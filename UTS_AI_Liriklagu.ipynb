{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **UTS Kecerdasan Buatan** \n",
        "###**\"An Automatic lyrics maker based on text using Marchine Learning\"**\n",
        "\n",
        "**Nama : Siti Aisya**\n",
        "\n",
        "**Kelas : SK5A Indralaya**\n",
        "\n",
        "**Nim : 09011182025001**\n"
      ],
      "metadata": {
        "id": "7gvw3dwGFocm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uCjC7WsRu3ZA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(lyric, max_length=None): \n",
        "    lyric = lyric.lower().strip()\n",
        "    lyric = lyric.replace(\"<newline>\", \" <newline> \")\n",
        "    \n",
        "    lyric = re.sub(r\"([?.!,])\", r\" \\1 \", lyric)\n",
        "    lyric = re.sub(r'([\" \"]+)', \" \", lyric)\n",
        "    lyric = re.sub(r\"[^a-zA-Z?.!,<>]\", \" \", lyric) \n",
        "    lyric = lyric.strip()\n",
        "    \n",
        "    if max_length != None:\n",
        "        lyric = \" \".join(lyric.split(\" \")[:max_length])\n",
        "    \n",
        "    return \"<start> \" + lyric + \" <end>\" "
      ],
      "metadata": {
        "id": "KS9O89RLu9OW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"Aku... dan<newline> kamu\", 5) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "60SmX9CSwpnn",
        "outputId": "69a21b78-0b92-455b-d541-236a0070a4a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> aku . . . dan <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(filename, max_length=None): \n",
        "    dataset = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        dataset = json.loads(file.read())\n",
        "    preprocessed_lyric = [preprocess(song[\"lyric\"], max_length) for song in dataset if len(song[\"lyric\"]) > 10]\n",
        "    return preprocessed_lyric "
      ],
      "metadata": {
        "id": "lwdfeVp6wtwm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = create_dataset(\"lyric_bahasa.json\", 162) "
      ],
      "metadata": {
        "id": "XkCk-kCUw0RJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 0\n",
        "total = 0\n",
        "for d in dataset:\n",
        "    max_length = max(max_length, len(d.split(\" \")))\n",
        "    total += len(d.split(\" \"))\n",
        "def create_tokenizer(lyrics, num_words=None):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", num_words=num_words, oov_token=\"<unk>\")\n",
        "    tokenizer.fit_on_texts(lyrics)\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize(tokenizer, lyrics): \n",
        "    tensor = tokenizer.texts_to_sequences(lyrics)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor\n",
        "def load_dataset(filename, num_words, max_length):\n",
        "    dataset = create_dataset(filename, max_length)\n",
        "    tokenizer = create_tokenizer(dataset, num_words)\n",
        "    input_tensor = tokenize(tokenizer, dataset)\n",
        "    \n",
        "    return tokenizer, input_tensor "
      ],
      "metadata": {
        "id": "z09LQ8HDw3aa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, input_tensor = load_dataset(\"lyric_bahasa.json\", 10000, 160)\n",
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHhpSGzXycXF",
        "outputId": "1242913b-831e-4571-aeac-dcfe98e39059"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23181, 162)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000 + 1\n",
        "input_tensor[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUWId-33yut1",
        "outputId": "9d3b2119-896f-44d0-a2ec-87f77bb7f2dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  14,   16,   79,    9, 1254,    2,   16,  460,    9, 1092,    2,\n",
              "         16,  783,    9,   32,    2,   16,  723,    5, 2128,    2,   16,\n",
              "         40,    9, 1013,    2,  725,   25, 1424,    2,   16,  364,    9,\n",
              "        309,    2,    5, 1111, 4599,    2,   31,  597,  163,    2,   65,\n",
              "        737,    9,  447,    8,   65,  737,    9,  447,    2,   69, 1339,\n",
              "        156,    2,   70,   70,   75,  163,    8,   70,   70,   75,  163,\n",
              "          2,    4,    7,   22,    8,    3,    7,  408,    2,   94,    7,\n",
              "        195,    8,   94,    3,  260,    2,   31,  597,  163,    2,   65,\n",
              "        737,    9,  447,    8,   65,  737,    9,  447,    2,   69, 1339,\n",
              "        156,    2,   70,   70,   75,  163,    8,   70,   70,   75,  163,\n",
              "          2,    4,    7,   22,    8,    3,    7,  408,    2,   94,    7,\n",
              "        195,    8,   94,    3,  260,    2,   16,   40,    9, 1013,    8,\n",
              "        725,   25, 1424,    2,   16,  364,    9,  309,    5, 1111, 4599,\n",
              "         15,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in input_tensor[0]:\n",
        "    if t == 0:\n",
        "        continue\n",
        "    print(t, \"=>\", tokenizer.index_word[t])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaCOnj0Ny1dz",
        "outputId": "5c947ed1-d3dd-45b3-9bdd-d04e147796f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 => <start>\n",
            "16 => ada\n",
            "79 => rindu\n",
            "9 => di\n",
            "1254 => malamku\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "460 => resah\n",
            "9 => di\n",
            "1092 => tidurku\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "783 => tangis\n",
            "9 => di\n",
            "32 => hatiku\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "723 => hasrat\n",
            "5 => yang\n",
            "2128 => menggebu\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "40 => engkau\n",
            "9 => di\n",
            "1013 => anganku\n",
            "2 => <newline>\n",
            "725 => bermain\n",
            "25 => dalam\n",
            "1424 => khayalku\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "364 => senyum\n",
            "9 => di\n",
            "309 => mataku\n",
            "2 => <newline>\n",
            "5 => yang\n",
            "1111 => menyiksa\n",
            "4599 => pandanganku\n",
            "2 => <newline>\n",
            "31 => ingin\n",
            "597 => berjumpa\n",
            "163 => denganmu\n",
            "2 => <newline>\n",
            "65 => walau\n",
            "737 => sekedar\n",
            "9 => di\n",
            "447 => mimpiku\n",
            "8 => ,\n",
            "65 => walau\n",
            "737 => sekedar\n",
            "9 => di\n",
            "447 => mimpiku\n",
            "2 => <newline>\n",
            "69 => sampai\n",
            "1339 => kapankah\n",
            "156 => menunggu\n",
            "2 => <newline>\n",
            "70 => hari\n",
            "70 => hari\n",
            "75 => indah\n",
            "163 => denganmu\n",
            "8 => ,\n",
            "70 => hari\n",
            "70 => hari\n",
            "75 => indah\n",
            "163 => denganmu\n",
            "2 => <newline>\n",
            "4 => aku\n",
            "7 => tak\n",
            "22 => bisa\n",
            "8 => ,\n",
            "3 => ku\n",
            "7 => tak\n",
            "408 => kuasa\n",
            "2 => <newline>\n",
            "94 => lama\n",
            "7 => tak\n",
            "195 => bertemu\n",
            "8 => ,\n",
            "94 => lama\n",
            "3 => ku\n",
            "260 => tanpamu\n",
            "2 => <newline>\n",
            "31 => ingin\n",
            "597 => berjumpa\n",
            "163 => denganmu\n",
            "2 => <newline>\n",
            "65 => walau\n",
            "737 => sekedar\n",
            "9 => di\n",
            "447 => mimpiku\n",
            "8 => ,\n",
            "65 => walau\n",
            "737 => sekedar\n",
            "9 => di\n",
            "447 => mimpiku\n",
            "2 => <newline>\n",
            "69 => sampai\n",
            "1339 => kapankah\n",
            "156 => menunggu\n",
            "2 => <newline>\n",
            "70 => hari\n",
            "70 => hari\n",
            "75 => indah\n",
            "163 => denganmu\n",
            "8 => ,\n",
            "70 => hari\n",
            "70 => hari\n",
            "75 => indah\n",
            "163 => denganmu\n",
            "2 => <newline>\n",
            "4 => aku\n",
            "7 => tak\n",
            "22 => bisa\n",
            "8 => ,\n",
            "3 => ku\n",
            "7 => tak\n",
            "408 => kuasa\n",
            "2 => <newline>\n",
            "94 => lama\n",
            "7 => tak\n",
            "195 => bertemu\n",
            "8 => ,\n",
            "94 => lama\n",
            "3 => ku\n",
            "260 => tanpamu\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "40 => engkau\n",
            "9 => di\n",
            "1013 => anganku\n",
            "8 => ,\n",
            "725 => bermain\n",
            "25 => dalam\n",
            "1424 => khayalku\n",
            "2 => <newline>\n",
            "16 => ada\n",
            "364 => senyum\n",
            "9 => di\n",
            "309 => mataku\n",
            "5 => yang\n",
            "1111 => menyiksa\n",
            "4599 => pandanganku\n",
            "15 => <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_tensor = sequence[:-1]\n",
        "    target_tensor = sequence[1:]\n",
        "    return input_tensor, target_tensor"
      ],
      "metadata": {
        "id": "2c6J7ceZzKD5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target([\"saya\", \"dan\", \"dia\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7LQ9HnXziFu",
        "outputId": "ed699238-4941-49e2-aa68-b985b31d96ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['saya', 'dan'], ['dan', 'dia'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(BUFFER_SIZE).map(split_input_target)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "NbmV0fS8zne3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-LCkWiuz6dT",
        "outputId": "b4e3faa8-fa4f-416d-c246-d3d83f68bd22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 161]), TensorShape([64, 161]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.GRU(units, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GRU(units, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "FF--5_Suz9F_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASh0P4390A93",
        "outputId": "9cf1638a-6f5b-41d3-8806-4ec25c0842c5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         2560256   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 1024)        3938304   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 1024)        0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, None, 1024)        6297600   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 1024)        0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 10001)       10251025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,047,185\n",
            "Trainable params: 23,047,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}.h5')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "0crj_GQt0EDr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "0lXCNzXA0ZzG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "XaN_269F0fYS",
        "outputId": "8591ed0e-332b-4a5d-edc1-cb95fb6005b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  2/362 [..............................] - ETA: 3:31:16 - loss: 9.1867 - sparse_categorical_accuracy: 0.1190    "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5a0070c96b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"checkpoint.h5\")"
      ],
      "metadata": {
        "id": "aG1g3qYn0isN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "6Krd6PAK5zhu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(seed, max_length=150):\n",
        "    start = seed.strip()\n",
        "    sequences = [tokenizer.word_index[i] for i in start.lower().split(\" \")]\n",
        "    for i in range(max_length):\n",
        "        x = np.array([sequences])\n",
        "        pred = model.predict(x)\n",
        "        pred_id = np.argmax(pred[0][-1])\n",
        "        if pred_id == 0 or pred_id == tokenizer.word_index[\"<end>\"]:\n",
        "            break\n",
        "        sequences.append(pred_id)\n",
        "    print_sequence(sequences)"
      ],
      "metadata": {
        "id": "gcqhKZz_57Tm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sequence(sequences):\n",
        "    result = []\n",
        "    for seq in sequences:\n",
        "        if seq == 0:\n",
        "            continue\n",
        "        word = tokenizer.index_word[seq]\n",
        "        if word == \"<start>\" or word == \"<end>\" or word == \"<unk>\":\n",
        "            word = \"\"\n",
        "        elif word == \"<newline>\":\n",
        "            word = \"\\n\"\n",
        "        result.append(word)\n",
        "    print(\" \".join(result))"
      ],
      "metadata": {
        "id": "zZCXVCiG6A2L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_search(\"<start> hujan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq3yaSL86FVo",
        "outputId": "7d8a1422-6f86-439e-a55f-42ea0438f28b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " hujan beta erti , luka tak juga duda \n",
            " hangati melayang , hatimu televisi \n",
            " hai ini coba \n",
            " tapi jangan kau lupa indah kasih turun \n",
            " ayo yang sekarang patah kau rasa \n",
            " pasti kan ada tinggal \n",
            " tak akan jiwaku pintaku ini \n",
            " di milikku  kan ada pelangi \n",
            " niat cemburu kan terhebat hati \n",
            " jangan ayo lagi \n",
            " hujan beta erti , luka tak juga duda \n",
            " hangati melayang , hatimu televisi \n",
            " hai ini coba \n",
            " tapi jangan kau lupa indah kasih turun \n",
            " ayo yang sekarang patah kau rasa \n",
            " pasti kan ada tinggal \n",
            " tak akan jiwaku pintaku ini , di milikku  kan ada pelangi \n",
            " niat cemburu kan terhebat hati , jangan ayo lagi \n",
            " tak akan jiwaku pintaku ini , di milikku  kan ada pelangi \n",
            " niat cemburu kan terhebat hati , jangan ayo lagi \n",
            " hujan kan tenggelam ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(seed, k=3, maxsample=150):\n",
        "    start = seed.strip()\n",
        "    pattern = [tokenizer.word_index[w] for w in start.lower().split(\" \")]\n",
        "    x = np.array([pattern])\n",
        "\n",
        "    # shape (1, n, vocab_size)\n",
        "    preds = model.predict(x)\n",
        "    # shape (k)\n",
        "    pred_ids = preds.argsort(axis=2)[0, -1, -k:][::-1]\n",
        "    # shape (vocab_size)\n",
        "    pred_scores = np.log(preds[0][-1])\n",
        "    # shape (k, n+1)\n",
        "    k_prev_words = [pattern + [id] for id in pred_ids]\n",
        "    # shape (k, n+1)\n",
        "    top_k_scores = [pred_scores[i] for i in pred_ids]\n",
        "\n",
        "    completed_sequences = []\n",
        "\n",
        "    for i in range(maxsample):\n",
        "        # shape (k, t, vocab_size)\n",
        "        preds = model.predict(k_prev_words)\n",
        "        # shape (k, k)\n",
        "        pred_ids = preds.argsort(axis=2)[:, -1, -k:][::-1]\n",
        "        # shape (k, vocab_size)\n",
        "        pred_scores = np.log(preds[:, -1])\n",
        "        pred_scores = [sc[idx] for idx, sc in zip(pred_ids, pred_scores)]\n",
        "        pred_scores = np.array(pred_scores)\n",
        "        top_k_preds = (top_k_scores + pred_scores.T).T\n",
        "        top_score = top_k_preds.flatten().argsort()[::-1][:k]\n",
        "\n",
        "        prev_words = [s//k for s in top_score]\n",
        "        next_words = [s%k for s in top_score]\n",
        "\n",
        "        top_k_scores = top_k_preds.flatten()[top_score]\n",
        "\n",
        "\n",
        "        k_candidate_words = [pred_ids[p][n] for p, n in zip(prev_words, next_words)]\n",
        "        k_prev_words = [k_prev_words[p] + [pred_ids[p][n]] for p, n in zip(prev_words, next_words)]\n",
        "        for j, token in enumerate(k_candidate_words):\n",
        "            if token == tokenizer.word_index[\"<end>\"]:\n",
        "                completed_sequences.append({\"seqs\": k_prev_words[j], \"score\": top_k_scores[j]})\n",
        "        if len(completed_sequences) == k:\n",
        "            break\n",
        "    completed_sequences = sorted(completed_sequences, key=lambda x: x['score'], reverse=True)\n",
        "    print_sequence(completed_sequences[0]['seqs'])"
      ],
      "metadata": {
        "id": "HRZvgqJI6Jr-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(\"<start> aku disini\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOnPjPYz6uim",
        "outputId": "c16ab96b-6e87-49e1-c188-8d559ea41b39"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " aku disini lagi , kau bersedih lagi \n",
            " ku tak mengerti mengapa berlalu , \n",
            " mengapa kau pergi pergi , bukan karena senyummu \n",
            " ku tak rela bila kau pergi \n",
            " aku tak rela , ku rela \n",
            " kau pergi janganlah \n",
            " aku tak rela bila kau jauh \n",
            " mungkin tak ada cinta \n",
            " aku tak rela bila kau pergi \n",
            " aku ingin engkau kembali \n",
            " ku tak rela kau tinggalkan \n",
            " diriku yang dulu tak ada \n",
            " karena kau bukan lagi \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(\"<start> disini aku masih sendiri\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ8ez0wI60JM",
        "outputId": "5a640026-353b-45a0-f206-c7b33cdfd85b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " disini aku masih sendiri , ada di sini dalam kutahu \n",
            " berbintang rasa dalam temukan , dalam hati \n",
            " dalam sepi ku dada kamu \n",
            " adakah ada di sini dalam tulis \n",
            " dalam rindu ku ingin bersamamu , bahagia bersamamu \n",
            " bahagia terucap , bahagia dalam hidupku \n",
            " bersamamu aku sementara \n",
            " bersamamu aku isi , ku bahagia ku bahagia \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(\"<start> kamu pergi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nUGNP8L7cCG",
        "outputId": "7fb2bd4e-951e-43cc-c064-b44daf73dcae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " kamu pergi , ku pergi \n",
            " pergi tinggalkan dirimu \n",
            " ku ingin kau tahu \n",
            " betapa ku mencintaimu , \n",
            " puas tolonglah ini , tak ada kamu \n",
            " kamu tak bisa , aku tak bisa , aku tak mampu \n",
            " \n",
            " ku tak bisa , tak bisa , tak bisa \n",
            " ku jaga , ku tak bisa , ku takkan bisa \n",
            " untuk bisa dingin \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_search(\"<start> aku dan kamu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyt_uoCx9YGc",
        "outputId": "aad995b2-6938-464e-c36c-2a478f5c7538"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " aku dan kamu saling cinta \n",
            " bukan hanya perlahan kata \n",
            " yang tak pernah bisa menyakiti cinta \n",
            " menyakiti semua rasa yang salah \n",
            " saat ini tak juga kau menyakiti \n",
            " namun rasa ini juga bisa \n",
            " kau berikan semua yang pernah ku rasa \n",
            " rasa cinta yang takkan pernah bisa \n",
            " ku berikan semua yang ku rasa \n",
            " aku yang kau sayang meski tak bisa kau rasa \n",
            " hidup ku tahu kau bukan untukku \n",
            " tapi rasa ini rasa sayang ini untukmu \n",
            " aku tahu ini takkan bisa \n",
            " menjadi padamu yang kau lepas \n",
            " sesaat saja kan ku beri \n",
            " semua yang ku rasa \n",
            " berdiri sayang seperti \n",
            " aku yang kan selalu tidak \n",
            " hanya untuk dirimu \n",
            " yang ku berikan hanya untukmu \n",
            " aku untukmu , berdiri sayang \n",
            " mimpi kau ingin menyakiti rasa \n",
            " yang aku cinta seperti \n",
            " meski kau tak pernah tahu \n",
            " rasa ini kan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(\"<start> engkau \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik7nEK5KGZRR",
        "outputId": "930ee1a3-0d5b-4611-bcdf-8935c7a825f0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " engkau datang padaku \n",
            " kala  gelap hatiku \n",
            " engkau datang padaku \n",
            " membawa luka yang kini ada \n",
            " \n",
            " jangan biarkan rindu yang terluka \n",
            " jangan datang lalu datang \n",
            " \n",
            " aku takkan ada \n",
            " perhatianku kasih kita coba \n"
          ]
        }
      ]
    }
  ]
}